---
layout: post
title:  "SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation"
date:   2024-09-19 10:00:00
categories: research
description: "Closed-Loop Scenario Generation"
author: "Benjamin Stoler"
published: true
sidebar:  false
hide_hero: true
permalink: /seal/
image: /assets/imgs/posts/2024-09-19-SEAL/main.png
link-new-tab: true
comments: false
---
<hr>

[Benjamin Stoler](https://benstoler.com), [Ingrid Navarro](https://navars.xyz), Jonathan Francis and [Jean Oh](https://cmubig.github.io/team/jean_oh/) 

<a class="button" itemprop="github" href="https://github.com/cmubig/SEAL" target="_blank">
  <i class="fab fa-github fa-lg"></i>
</a>
<a class="button" itemprop="paper" href="https://arxiv.org/abs/2409.10320" target="_blank">
  <i class="fas fa-file fa-lg"></i>    
</a>

# Abstract

Verification and validation of autonomous driving (AD) systems and components is of increasing 
importance, as such technology increases in real-world prevalence. Safety-critical scenario generation 
is a key approach to robustify AD policies through closed-loop training. However, existing approaches 
for scenario generation rely on simplistic objectives, resulting in overly-aggressive or non-reactive 
adversarial behaviors. To generate diverse adversarial yet realistic scenarios, we propose 
**Skill-Enabled Adversary Learning (SEAL)**, a scenario perturbation approach which leverages learned 
scoring functions and adversarial, human-like skills. SEAL-perturbed scenarios are more realistic than 
SOTA baselines, leading to improved ego task success across real-world, in-distribution, and 
out-of-distribution scenarios, of more than 20%. 

<p align="center">
  <img width="1280" src="/assets/imgs/posts/2024-09-19-SEAL/main.png" alt="Fairness">
</p>

<hr>

# Motivation

## Scenario Realism

State-of-the-art (SOTA) adversarial scenario-generation approaches often struggle to provide 
<i>useful</i> training stimuli to closed-loop agents. Specifically, we identify that recent SOTA
approaches generally limited view of safety-criticality, often focused on optimizing **unrealistic** and **overly-aggressive** adversarial behavior, while also lacking reactivity to an ego-agent's behavior diversity. 

<p align="center">
<video width="400" height="240" autoplay loop muted>
  <source src="/assets/imgs/posts/2024-09-19-SEAL/unrealistic1.mp4" type="video/mp4" />
</video>
<video width="400" height="240" autoplay loop muted>
  <source src="/assets/imgs/posts/2024-09-19-SEAL/unrealistic2.mp4" type="video/mp4" />
</video>
</p>
<p align="center">
<caption><i>Examples of unrealistic, overly aggressive adversarial agents in SOTA approaches.</i></caption>
</p>

### <img width="50" src="/assets/imgs/posts/2024-09-19-SEAL/light-bulb.png" alt="bulb"> Our Idea

To address these limitations, we propose a method for **Skill-Enabled Adversary Learning (SEAL)** 
which improves downstream ego behavior in closed loop training for safety-critical scenario generation.

**SEAL** introduces two novel components:
1. A **learned scoring function** to anticipate how a reactive ego agent will respond to a candidate adversarial behavior. 
2. A **reactive** adversary policy that hierarchically selects human-like skill primitives to increase criticality and maintain realism.

## Evaluation Fairness

Safety-critical scenario generation approaches should be evaluated not only in terms of **induced criticality**, but also in terms of behavior **realism**. However, recent works 
rely on evaluating ego policies leveraging their scenario generation approach, wherein safety-critical
behavior is <u>in-distribution</u>. While this is informative for assessing ego performance, we argue that performance on challenging scenes is ultimately more important.

### <img width="50" src="/assets/imgs/posts/2024-09-19-SEAL/light-bulb.png" alt="bulb"> Our Idea

We create a realistic <u>out-of-distribution</u> evaluation setting leveraging recent work on [scenario characterization](https://navars.xyz/safeshift/) to identify real (non-generated) safety-relevant 
scenarios. 


<hr>

# Method

### Learned Score Function

todo

### Adversarial Skill Learning

<p align="center">
  <img width="500" src="/assets/imgs/posts/2024-09-19-SEAL/skill_space.png" alt="Skills">
</p>

# Results

#### Ego Policy Training

todo

#### SEAL-generated Scenario Quality 

todo

<hr>

**Check out our paper for more details!**

# BibTeX

```
@article{stoler2024seal,
  title={SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation},
  author={Stoler, Benjamin and Navarro, Ingrid and Francis, Jonathan and Oh, Jean},
  journal={arXiv preprint arXiv:2409.10320},
  year={2024}
}
```

<hr>